{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SBsCRqNAEbJD"
      },
      "source": [
        "# Lab:  Logistic Regression for Gene Expression Data\n",
        "\n",
        "In this lab, we use logistic regression to predict biological characteristics (\"phenotypes\") from gene expression data.  In addition to the concepts in [breast cancer demo](./breast_cancer.ipynb), you will learn to:\n",
        "* Handle missing data\n",
        "* Perform multi-class logistic classification\n",
        "* Create a confusion matrix\n",
        "* Use L1-regularization for improved estimation in the case of sparse weights (Grad students only)\n",
        "\n",
        "## Background\n",
        "\n",
        "Genes are the basic unit in the DNA and encode blueprints for proteins.  When proteins are synthesized from a gene, the gene is said to \"express\".  Micro-arrays are devices that measure the expression levels of large numbers of genes in parallel.  By finding correlations between expression levels and phenotypes, scientists can identify possible genetic markers for biological characteristics.\n",
        "\n",
        "The data in this lab comes from:\n",
        "\n",
        "https://archive.ics.uci.edu/ml/datasets/Mice+Protein+Expression\n",
        "\n",
        "In this data, mice were characterized by three properties:\n",
        "* Whether they had down's syndrome (trisomy) or not\n",
        "* Whether they were stimulated to learn or not\n",
        "* Whether they had a drug memantine or a saline control solution.\n",
        "\n",
        "With these three choices, there are 8 possible classes for each mouse.  For each mouse, the expression levels were measured across 77 genes.  We will see if the characteristics can be predicted from the gene expression levels.  This classification could reveal which genes are potentially involved in Down's syndrome and if drugs and learning have any noticeable effects.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CN_QdJ8HEbJG"
      },
      "source": [
        "## Load the Data\n",
        "\n",
        "We begin by loading the standard modules."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "1Ll_1gsOEbJG"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib\n",
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline\n",
        "from sklearn import linear_model, preprocessing"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nHR63WgwEbJH"
      },
      "source": [
        "Use the `pd.read_excel` command to read the data from \n",
        "\n",
        "https://archive.ics.uci.edu/ml/machine-learning-databases/00342/Data_Cortex_Nuclear.xls\n",
        "\n",
        "into a dataframe `df`.  Use the `index_col` option to specify that column 0 is the index.  Use the `df.head()` to print the first few rows."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-L_RxV8tEbJH",
        "outputId": "a9113429-cde8-42f2-8d62-933961d18101"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(1080, 82)"
            ]
          },
          "metadata": {},
          "execution_count": 2
        }
      ],
      "source": [
        "# TODO\n",
        "df = pd.read_excel('https://archive.ics.uci.edu/ml/machine-learning-databases/00342/Data_Cortex_Nuclear.xls')\n",
        "df.shape"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EuSEG6XYEbJI"
      },
      "source": [
        "This data has missing values.  The site:\n",
        "\n",
        "http://pandas.pydata.org/pandas-docs/stable/missing_data.html\n",
        "\n",
        "has an excellent summary of methods to deal with missing values.  Following the techniques there, create a new data frame `df1` where the missing values in each column are filled with the mean values from the non-missing values."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df[40:75]['pCFOS_N']"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xbiS3wYeHKC_",
        "outputId": "8d282004-14fe-4687-f954-bccb329f9260"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "40    0.132431\n",
              "41    0.121586\n",
              "42    0.109086\n",
              "43    0.123425\n",
              "44    0.124575\n",
              "45         NaN\n",
              "46         NaN\n",
              "47         NaN\n",
              "48         NaN\n",
              "49         NaN\n",
              "50         NaN\n",
              "51         NaN\n",
              "52         NaN\n",
              "53         NaN\n",
              "54         NaN\n",
              "55         NaN\n",
              "56         NaN\n",
              "57         NaN\n",
              "58         NaN\n",
              "59         NaN\n",
              "60    0.127805\n",
              "61    0.130997\n",
              "62    0.122870\n",
              "63    0.132692\n",
              "64    0.140960\n",
              "65    0.127291\n",
              "66    0.132730\n",
              "67    0.142773\n",
              "68    0.130793\n",
              "69    0.143351\n",
              "70    0.148772\n",
              "71    0.148578\n",
              "72    0.157487\n",
              "73    0.168545\n",
              "74    0.164195\n",
              "Name: pCFOS_N, dtype: float64"
            ]
          },
          "metadata": {},
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3YfXzrqOEbJI",
        "outputId": "1c28ea00-68b3-4707-f8b8-5420c31a070e"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "40    0.132431\n",
              "41    0.121586\n",
              "42    0.109086\n",
              "43    0.123425\n",
              "44    0.124575\n",
              "45    0.131053\n",
              "46    0.131053\n",
              "47    0.131053\n",
              "48    0.131053\n",
              "49    0.131053\n",
              "50    0.131053\n",
              "51    0.131053\n",
              "52    0.131053\n",
              "53    0.131053\n",
              "54    0.131053\n",
              "55    0.131053\n",
              "56    0.131053\n",
              "57    0.131053\n",
              "58    0.131053\n",
              "59    0.131053\n",
              "60    0.127805\n",
              "61    0.130997\n",
              "62    0.122870\n",
              "63    0.132692\n",
              "64    0.140960\n",
              "65    0.127291\n",
              "66    0.132730\n",
              "67    0.142773\n",
              "68    0.130793\n",
              "69    0.143351\n",
              "70    0.148772\n",
              "71    0.148578\n",
              "72    0.157487\n",
              "73    0.168545\n",
              "74    0.164195\n",
              "Name: pCFOS_N, dtype: float64"
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ],
      "source": [
        "# TODO\n",
        "df1 = df.where(pd.notna(df), df.mean(numeric_only = True), axis = \"columns\")\n",
        "df1[40:75]['pCFOS_N']"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "D7Os0kdrEbJI"
      },
      "source": [
        "## Binary Classification for Down's Syndrome\n",
        "\n",
        "We will first predict the binary class label in `df1['Genotype']` which indicates if the mouse has Down's syndrome or not.  Get the string values in `df1['Genotype'].values` and convert this to a numeric vector `y` with 0 or 1.  You may wish to use the `np.unique` command with the `return_inverse=True` option."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "G-xZEWENEbJJ",
        "outputId": "5c50755a-22f4-47ba-8d0d-ecce6b8c1a89"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([0, 0, 0, ..., 1, 1, 1])"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ],
      "source": [
        "# TODO\n",
        "# model = linear_model.LinearRegression()\n",
        "y = np.unique(df1['Genotype'].values, return_inverse = True)[1]\n",
        "y"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lT2bme3kEbJJ"
      },
      "source": [
        "As predictors, get all but the last four columns of the dataframes.  Store the data matrix into `X` and the names of the columns in `xnames`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 444
        },
        "id": "AQUpZwsKEbJJ",
        "outputId": "db7d9655-858f-4822-bb27-659e4f91feb8"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['DYRK1A_N' 'ITSN1_N' 'BDNF_N' 'NR1_N' 'NR2A_N' 'pAKT_N' 'pBRAF_N'\n",
            " 'pCAMKII_N' 'pCREB_N' 'pELK_N' 'pERK_N' 'pJNK_N' 'PKCA_N' 'pMEK_N'\n",
            " 'pNR1_N' 'pNR2A_N' 'pNR2B_N' 'pPKCAB_N' 'pRSK_N' 'AKT_N' 'BRAF_N'\n",
            " 'CAMKII_N' 'CREB_N' 'ELK_N' 'ERK_N' 'GSK3B_N' 'JNK_N' 'MEK_N' 'TRKA_N'\n",
            " 'RSK_N' 'APP_N' 'Bcatenin_N' 'SOD1_N' 'MTOR_N' 'P38_N' 'pMTOR_N'\n",
            " 'DSCR1_N' 'AMPKA_N' 'NR2B_N' 'pNUMB_N' 'RAPTOR_N' 'TIAM1_N' 'pP70S6_N'\n",
            " 'NUMB_N' 'P70S6_N' 'pGSK3B_N' 'pPKCG_N' 'CDK5_N' 'S6_N' 'ADARB1_N'\n",
            " 'AcetylH3K9_N' 'RRP1_N' 'BAX_N' 'ARC_N' 'ERBB4_N' 'nNOS_N' 'Tau_N'\n",
            " 'GFAP_N' 'GluR3_N' 'GluR4_N' 'IL1B_N' 'P3525_N' 'pCASP9_N' 'PSD95_N'\n",
            " 'SNCA_N' 'Ubiquitin_N' 'pGSK3B_Tyr216_N' 'SHH_N' 'BAD_N' 'BCL2_N' 'pS6_N'\n",
            " 'pCFOS_N' 'SYP_N' 'H3AcK18_N' 'EGR1_N' 'H3MeK4_N' 'CaNA_N']\n",
            "(1080, 77)\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "   DYRK1A_N   ITSN1_N    BDNF_N     NR1_N    NR2A_N    pAKT_N   pBRAF_N  \\\n",
              "0  0.503644  0.747193  0.430175  2.816329  5.990152  0.218830  0.177565   \n",
              "1  0.514617  0.689064  0.411770  2.789514  5.685038  0.211636  0.172817   \n",
              "2  0.509183  0.730247  0.418309  2.687201  5.622059  0.209011  0.175722   \n",
              "3  0.442107  0.617076  0.358626  2.466947  4.979503  0.222886  0.176463   \n",
              "4  0.434940  0.617430  0.358802  2.365785  4.718679  0.213106  0.173627   \n",
              "\n",
              "   pCAMKII_N   pCREB_N    pELK_N  ...     SHH_N     BAD_N    BCL2_N     pS6_N  \\\n",
              "0   2.373744  0.232224  1.750936  ...  0.188852  0.122652  0.134762  0.106305   \n",
              "1   2.292150  0.226972  1.596377  ...  0.200404  0.116682  0.134762  0.106592   \n",
              "2   2.283337  0.230247  1.561316  ...  0.193685  0.118508  0.134762  0.108303   \n",
              "3   2.152301  0.207004  1.595086  ...  0.192112  0.132781  0.134762  0.103184   \n",
              "4   2.134014  0.192158  1.504230  ...  0.205604  0.129954  0.134762  0.104784   \n",
              "\n",
              "    pCFOS_N     SYP_N  H3AcK18_N    EGR1_N  H3MeK4_N    CaNA_N  \n",
              "0  0.108336  0.427099   0.114783  0.131790  0.128186  1.675652  \n",
              "1  0.104315  0.441581   0.111974  0.135103  0.131119  1.743610  \n",
              "2  0.106219  0.435777   0.111883  0.133362  0.127431  1.926427  \n",
              "3  0.111262  0.391691   0.130405  0.147444  0.146901  1.700563  \n",
              "4  0.110694  0.434154   0.118481  0.140314  0.148380  1.839730  \n",
              "\n",
              "[5 rows x 77 columns]"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-9c263bed-ec76-432d-973a-761ce2bf3766\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>DYRK1A_N</th>\n",
              "      <th>ITSN1_N</th>\n",
              "      <th>BDNF_N</th>\n",
              "      <th>NR1_N</th>\n",
              "      <th>NR2A_N</th>\n",
              "      <th>pAKT_N</th>\n",
              "      <th>pBRAF_N</th>\n",
              "      <th>pCAMKII_N</th>\n",
              "      <th>pCREB_N</th>\n",
              "      <th>pELK_N</th>\n",
              "      <th>...</th>\n",
              "      <th>SHH_N</th>\n",
              "      <th>BAD_N</th>\n",
              "      <th>BCL2_N</th>\n",
              "      <th>pS6_N</th>\n",
              "      <th>pCFOS_N</th>\n",
              "      <th>SYP_N</th>\n",
              "      <th>H3AcK18_N</th>\n",
              "      <th>EGR1_N</th>\n",
              "      <th>H3MeK4_N</th>\n",
              "      <th>CaNA_N</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0.503644</td>\n",
              "      <td>0.747193</td>\n",
              "      <td>0.430175</td>\n",
              "      <td>2.816329</td>\n",
              "      <td>5.990152</td>\n",
              "      <td>0.218830</td>\n",
              "      <td>0.177565</td>\n",
              "      <td>2.373744</td>\n",
              "      <td>0.232224</td>\n",
              "      <td>1.750936</td>\n",
              "      <td>...</td>\n",
              "      <td>0.188852</td>\n",
              "      <td>0.122652</td>\n",
              "      <td>0.134762</td>\n",
              "      <td>0.106305</td>\n",
              "      <td>0.108336</td>\n",
              "      <td>0.427099</td>\n",
              "      <td>0.114783</td>\n",
              "      <td>0.131790</td>\n",
              "      <td>0.128186</td>\n",
              "      <td>1.675652</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>0.514617</td>\n",
              "      <td>0.689064</td>\n",
              "      <td>0.411770</td>\n",
              "      <td>2.789514</td>\n",
              "      <td>5.685038</td>\n",
              "      <td>0.211636</td>\n",
              "      <td>0.172817</td>\n",
              "      <td>2.292150</td>\n",
              "      <td>0.226972</td>\n",
              "      <td>1.596377</td>\n",
              "      <td>...</td>\n",
              "      <td>0.200404</td>\n",
              "      <td>0.116682</td>\n",
              "      <td>0.134762</td>\n",
              "      <td>0.106592</td>\n",
              "      <td>0.104315</td>\n",
              "      <td>0.441581</td>\n",
              "      <td>0.111974</td>\n",
              "      <td>0.135103</td>\n",
              "      <td>0.131119</td>\n",
              "      <td>1.743610</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>0.509183</td>\n",
              "      <td>0.730247</td>\n",
              "      <td>0.418309</td>\n",
              "      <td>2.687201</td>\n",
              "      <td>5.622059</td>\n",
              "      <td>0.209011</td>\n",
              "      <td>0.175722</td>\n",
              "      <td>2.283337</td>\n",
              "      <td>0.230247</td>\n",
              "      <td>1.561316</td>\n",
              "      <td>...</td>\n",
              "      <td>0.193685</td>\n",
              "      <td>0.118508</td>\n",
              "      <td>0.134762</td>\n",
              "      <td>0.108303</td>\n",
              "      <td>0.106219</td>\n",
              "      <td>0.435777</td>\n",
              "      <td>0.111883</td>\n",
              "      <td>0.133362</td>\n",
              "      <td>0.127431</td>\n",
              "      <td>1.926427</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>0.442107</td>\n",
              "      <td>0.617076</td>\n",
              "      <td>0.358626</td>\n",
              "      <td>2.466947</td>\n",
              "      <td>4.979503</td>\n",
              "      <td>0.222886</td>\n",
              "      <td>0.176463</td>\n",
              "      <td>2.152301</td>\n",
              "      <td>0.207004</td>\n",
              "      <td>1.595086</td>\n",
              "      <td>...</td>\n",
              "      <td>0.192112</td>\n",
              "      <td>0.132781</td>\n",
              "      <td>0.134762</td>\n",
              "      <td>0.103184</td>\n",
              "      <td>0.111262</td>\n",
              "      <td>0.391691</td>\n",
              "      <td>0.130405</td>\n",
              "      <td>0.147444</td>\n",
              "      <td>0.146901</td>\n",
              "      <td>1.700563</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>0.434940</td>\n",
              "      <td>0.617430</td>\n",
              "      <td>0.358802</td>\n",
              "      <td>2.365785</td>\n",
              "      <td>4.718679</td>\n",
              "      <td>0.213106</td>\n",
              "      <td>0.173627</td>\n",
              "      <td>2.134014</td>\n",
              "      <td>0.192158</td>\n",
              "      <td>1.504230</td>\n",
              "      <td>...</td>\n",
              "      <td>0.205604</td>\n",
              "      <td>0.129954</td>\n",
              "      <td>0.134762</td>\n",
              "      <td>0.104784</td>\n",
              "      <td>0.110694</td>\n",
              "      <td>0.434154</td>\n",
              "      <td>0.118481</td>\n",
              "      <td>0.140314</td>\n",
              "      <td>0.148380</td>\n",
              "      <td>1.839730</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>5 rows Ã— 77 columns</p>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-9c263bed-ec76-432d-973a-761ce2bf3766')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-9c263bed-ec76-432d-973a-761ce2bf3766 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-9c263bed-ec76-432d-973a-761ce2bf3766');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ],
      "source": [
        "# TODO\n",
        "X = df1.iloc[:,1:-4]\n",
        "xnames = X.columns.values\n",
        "print(xnames)\n",
        "print(X.shape)\n",
        "X.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MnAVE7F2EbJK"
      },
      "source": [
        "Split the data into training and test with 30% allocated for test.  You can use the train"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "U76XHmdjEbJK",
        "outputId": "7391eec2-6f49-4e1d-a191-a94f4d0efa87"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(756, 77)\n",
            "(324, 77)\n",
            "(756,)\n",
            "(324,)\n"
          ]
        }
      ],
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# TODO:\n",
        "Xtr, Xts, ytr, yts = train_test_split(X, y, test_size = 0.3, random_state = 77)\n",
        "print(Xtr.shape)\n",
        "print(Xts.shape)\n",
        "print(ytr.shape)\n",
        "print(yts.shape)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HcV1zEPvEbJK"
      },
      "source": [
        "Scale the data with the `StandardScaler`.  Store the scaled values in `Xtr1` and `Xts1`. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "AS13vk60EbJK"
      },
      "outputs": [],
      "source": [
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "# TODO\n",
        "def scale(Xtr, Xts):\n",
        "  scaler = StandardScaler()\n",
        "  return scaler.fit_transform(Xtr), scaler.transform(Xts)\n",
        "\n",
        "Xtr1, Xts1 = scale(Xtr, Xts)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Sc-do3cOEbJK"
      },
      "source": [
        "Create a `LogisticRegression` object `logreg` and `fit` on the scaled training data.  Set the regularization level to `C=1e5` and use the optimizer `solver=liblinear`. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Q0K_a0lNEbJL",
        "outputId": "2a46acd7-9568-47ba-bba0-44767a0a39a9"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "LogisticRegression(C=100000.0, max_iter=1000)"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ],
      "source": [
        "# TODO\n",
        "logreg = linear_model.LogisticRegression(C = 1e5, solver = 'lbfgs', max_iter = 1000)\n",
        "logreg.fit(Xtr1, ytr)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sdm4zGmVEbJL"
      },
      "source": [
        "Measure the accuracy of the classifer on test data.  You should get around 94%.  "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CZjGhrS1EbJL",
        "outputId": "a3e86df7-aaa2-4d08-cbaf-755af5a9f895"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy = 95.37 %\n"
          ]
        }
      ],
      "source": [
        "# TODO\n",
        "yhat = logreg.predict(Xts1)\n",
        "print(f'Accuracy = {round(logreg.score(Xts1, yts) * 100, 2)} %')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dkM3KiqqEbJL"
      },
      "source": [
        "## Interpreting the weight vector"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "q5iI2InGEbJL"
      },
      "source": [
        "Create a stem plot of the coefficients, `W` in the logistic regression model.  Jse the `plt.stem()` function with the `use_line_collection=True` option.  You can get the coefficients from `logreg.coef_`, but you will need to reshape this to a 1D array.  "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 282
        },
        "id": "J__d5uJBEbJL",
        "outputId": "39977b4e-661d-49ab-e908-591129e77028"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<StemContainer object of 3 artists>"
            ]
          },
          "metadata": {},
          "execution_count": 11
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ],
      "source": [
        "# TODO\n",
        "W = logreg.coef_.reshape(-1, 1)[:, 0]\n",
        "plt.stem(W, use_line_collection = True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WKAFhJglEbJM"
      },
      "source": [
        "You should see that `W[i]` is very large for a few components `i`.  These are the genes that are likely to be most involved in Down's Syndrome.   Below we will use L1 regression to enforce sparsity.  Find the names of the genes for two components `i` where the magnitude of `W[i]` is largest.  "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "U1YvbTecEbJM",
        "outputId": "f5ea5a30-3242-444c-8302-6f2c5e397d0a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['ITSN1_N' 'APP_N']\n"
          ]
        }
      ],
      "source": [
        "# TODO\n",
        "W1 = sorted(W)\n",
        "print(xnames[(W == W1[-2]) + (W == W1[-1])])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yqL_t0C9EbJM"
      },
      "source": [
        "## Cross Validation\n",
        "\n",
        "To obtain a slightly more accurate result, now perform 10-fold cross validation and measure the average precision, recall and f1-score.  Note, that in performing the cross-validation, you will want to randomly permute the test and training sets using the `shuffle` option.  In this data set, all the samples from each class are bunched together, so shuffling is essential.  Print the mean precision, recall and f1-score and error rate across all the folds."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WL4Yr1y9EbJM",
        "outputId": "a8017f36-c130-4094-eb3c-5cb80d1b31a5"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Average of all precision  = 95.73 %\n",
            "Average of all recall     = 95.75 %\n",
            "Average of all f1         = 95.72 %\n",
            "Average of all Error Rate = 0.0426\n"
          ]
        }
      ],
      "source": [
        "from sklearn.model_selection import KFold\n",
        "from sklearn.metrics import precision_recall_fscore_support\n",
        "nfold = 10\n",
        "kf = KFold(n_splits = nfold, shuffle = True)\n",
        "\n",
        "# TODO\n",
        "all_precision = []\n",
        "all_recall = []\n",
        "all_fscore = []\n",
        "all_accuracy = []\n",
        "\n",
        "for xtr_split_indices, xts_split_indices in kf.split(X):\n",
        "  fXtr = X.iloc[xtr_split_indices, :]\n",
        "  fytr = y[xtr_split_indices]\n",
        "  fXts = X.iloc[xts_split_indices, :]\n",
        "  fyts = y[xts_split_indices]\n",
        "\n",
        "  Xtr1, Xts1 = scale(fXtr, fXts)\n",
        "  logreg = linear_model.LogisticRegression(C = 1e5, solver = 'lbfgs', max_iter = 1000)\n",
        "  logreg.fit(Xtr1, fytr)\n",
        "  yhat = logreg.predict(Xts1)\n",
        "\n",
        "  precision, recall, fscore, _ = precision_recall_fscore_support(fyts, yhat, average = 'macro')\n",
        "\n",
        "  all_precision.append(precision)\n",
        "  all_recall.append(recall)\n",
        "  all_fscore.append(fscore)\n",
        "  all_accuracy.append(np.mean(fyts == yhat))\n",
        "\n",
        "\n",
        "print(f'Average of all precision  = {round(np.mean(all_precision) * 100, 2)} %')\n",
        "print(f'Average of all recall     = {round(np.mean(all_recall) * 100, 2)} %')\n",
        "print(f'Average of all f1         = {round(np.mean(all_fscore) * 100, 2)} %')\n",
        "print(f'Average of all Error Rate = {round(1 - np.mean(all_accuracy) , 4)}')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mxifGUrYEbJM"
      },
      "source": [
        "## Multi-Class Classification\n",
        "\n",
        "Now use the response variable in `df1['class']`.  This has 8 possible classes.  Use the `np.unique` funtion as before to convert this to a vector `y` with values 0 to 7."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_80EEyakEbJM",
        "outputId": "e222ff73-fe48-4435-8bdc-16113b19aaf0"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([0, 0, 0, ..., 7, 7, 7])"
            ]
          },
          "metadata": {},
          "execution_count": 14
        }
      ],
      "source": [
        "# TODO\n",
        "y = np.unique(df1['class'], return_inverse = True)[1]\n",
        "y"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NvdVFn74EbJM"
      },
      "source": [
        "Fit a multi-class logistic model by creating a `LogisticRegression` object, `logreg` and then calling the `logreg.fit` method."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HcG1v2AXEbJM"
      },
      "source": [
        "Now perform 10-fold cross validation, and measure the confusion matrix `C` on the test data in each fold. You can use the `confustion_matrix` method in the `sklearn` package.  Add the confusion matrix counts across all folds and then normalize the rows of the confusion matrix so that they sum to one.  Thus, each element `C[i,j]` will represent the fraction of samples where `yhat==j` given `ytrue==i`.  Print the confusion matrix.  You can use the command\n",
        "\n",
        "    print(np.array_str(C, precision=4, suppress_small=True))\n",
        "    \n",
        "to create a nicely formatted print.  Also print the overall mean and SE of the test accuracy across the folds."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "I6Z0FFkZEbJN",
        "outputId": "66752497-7ef5-48f2-a55e-1fc2df939b1a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Average of all precision = 90.13 %\n",
            "Average of all recall    = 89.81 %\n",
            "Average of all f1        = 89.52 %\n",
            "Average of all accuracy  = 89.81 %\n",
            "SE      of     accuracy  = 0.01\n",
            "\n",
            "[[[0.89 0.15 0.   0.   0.   0.   0.   0.  ]\n",
            "  [0.26 0.46 0.   0.   0.18 0.   0.   0.  ]\n",
            "  [0.   0.   1.   0.   0.   0.   0.   0.  ]\n",
            "  [0.   0.   0.   0.93 0.   0.   0.   0.11]\n",
            "  [0.   0.08 0.   0.   0.91 0.   0.   0.  ]\n",
            "  [0.   0.   0.   0.   0.   1.   0.   0.  ]\n",
            "  [0.   0.   0.33 0.   0.   0.   0.8  0.  ]\n",
            "  [0.   0.   0.   0.   0.   0.   0.   1.  ]]\n",
            "\n",
            " [[0.71 0.23 0.   0.   0.08 0.   0.   0.  ]\n",
            "  [0.   0.92 0.   0.   0.08 0.   0.   0.  ]\n",
            "  [0.   0.   1.   0.   0.   0.   0.   0.  ]\n",
            "  [0.   0.   0.   0.94 0.   0.   0.   0.05]\n",
            "  [0.   0.   0.   0.   1.   0.   0.   0.  ]\n",
            "  [0.   0.   0.   0.   0.   1.   0.   0.  ]\n",
            "  [0.   0.   0.18 0.   0.   0.   0.71 0.  ]\n",
            "  [0.   0.   0.   0.   0.   0.   0.   1.  ]]\n",
            "\n",
            " [[0.75 0.19 0.   0.   0.   0.   0.   0.  ]\n",
            "  [0.06 0.76 0.   0.   0.2  0.17 0.   0.  ]\n",
            "  [0.   0.   0.94 0.   0.   0.   0.12 0.  ]\n",
            "  [0.   0.   0.11 0.89 0.   0.   0.   0.  ]\n",
            "  [0.   0.05 0.   0.   0.9  0.   0.   0.  ]\n",
            "  [0.   0.   0.   0.   0.   1.   0.   0.  ]\n",
            "  [0.   0.   0.06 0.   0.   0.   0.88 0.  ]\n",
            "  [0.   0.   0.   0.   0.   0.   0.   1.  ]]\n",
            "\n",
            " [[0.7  0.   0.   0.   0.19 0.   0.   0.  ]\n",
            "  [0.1  0.91 0.   0.   0.   0.   0.   0.  ]\n",
            "  [0.   0.   1.   0.   0.   0.   0.   0.  ]\n",
            "  [0.   0.   0.   1.   0.   0.   0.   0.  ]\n",
            "  [0.   0.18 0.   0.   0.88 0.   0.   0.  ]\n",
            "  [0.   0.   0.   0.   0.   1.   0.   0.  ]\n",
            "  [0.   0.   0.23 0.   0.   0.   0.86 0.  ]\n",
            "  [0.   0.   0.   0.   0.   0.   0.   1.  ]]\n",
            "\n",
            " [[0.87 0.12 0.   0.   0.08 0.   0.   0.  ]\n",
            "  [0.07 0.75 0.   0.   0.08 0.   0.   0.  ]\n",
            "  [0.   0.   1.   0.   0.   0.   0.   0.  ]\n",
            "  [0.   0.   0.   1.   0.   0.   0.   0.  ]\n",
            "  [0.   0.   0.   0.   0.92 0.09 0.   0.  ]\n",
            "  [0.   0.   0.   0.   0.   1.   0.   0.  ]\n",
            "  [0.   0.   0.06 0.   0.   0.   0.95 0.  ]\n",
            "  [0.   0.   0.   0.   0.   0.   0.   1.  ]]\n",
            "\n",
            " [[0.89 0.07 0.   0.   0.08 0.   0.   0.  ]\n",
            "  [0.11 0.73 0.   0.   0.17 0.   0.   0.  ]\n",
            "  [0.   0.   1.   0.   0.   0.   0.   0.  ]\n",
            "  [0.   0.   0.   0.85 0.   0.   0.   0.17]\n",
            "  [0.   0.07 0.   0.   0.83 0.   0.   0.08]\n",
            "  [0.   0.07 0.   0.   0.08 0.78 0.   0.  ]\n",
            "  [0.   0.   0.07 0.   0.   0.   0.93 0.  ]\n",
            "  [0.   0.   0.   0.   0.   0.   0.   1.  ]]\n",
            "\n",
            " [[0.5  0.33 0.   0.   0.05 0.   0.   0.  ]\n",
            "  [0.2  0.5  0.   0.   0.15 0.08 0.   0.  ]\n",
            "  [0.   0.   1.   0.   0.   0.   0.   0.  ]\n",
            "  [0.   0.   0.   0.94 0.   0.   0.   0.07]\n",
            "  [0.   0.33 0.   0.   0.8  0.   0.   0.  ]\n",
            "  [0.   0.   0.   0.   0.05 0.92 0.   0.  ]\n",
            "  [0.   0.   0.06 0.   0.   0.   0.83 0.  ]\n",
            "  [0.   0.   0.   0.   0.   0.   0.   1.  ]]\n",
            "\n",
            " [[0.88 0.14 0.   0.   0.   0.   0.   0.  ]\n",
            "  [0.06 0.79 0.   0.   0.1  0.07 0.   0.  ]\n",
            "  [0.   0.   1.   0.   0.   0.   0.   0.  ]\n",
            "  [0.   0.   0.   1.   0.   0.   0.   0.  ]\n",
            "  [0.   0.   0.   0.   0.9  0.   0.   0.06]\n",
            "  [0.   0.   0.   0.   0.   1.   0.   0.  ]\n",
            "  [0.   0.   0.17 0.14 0.   0.   0.67 0.  ]\n",
            "  [0.   0.   0.   0.   0.   0.   0.   1.  ]]\n",
            "\n",
            " [[0.87 0.14 0.   0.   0.   0.   0.   0.  ]\n",
            "  [0.13 0.86 0.   0.   0.   0.   0.   0.  ]\n",
            "  [0.   0.   1.   0.   0.   0.   0.   0.  ]\n",
            "  [0.   0.   0.   1.   0.   0.   0.   0.  ]\n",
            "  [0.   0.   0.   0.   0.86 0.29 0.   0.  ]\n",
            "  [0.   0.   0.   0.   0.   1.   0.   0.  ]\n",
            "  [0.   0.   0.   0.   0.   0.   1.   0.  ]\n",
            "  [0.   0.   0.   0.   0.   0.   0.   1.  ]]\n",
            "\n",
            " [[0.76 0.21 0.   0.   0.06 0.   0.   0.  ]\n",
            "  [0.06 0.79 0.   0.   0.   0.33 0.   0.  ]\n",
            "  [0.   0.   1.   0.   0.   0.   0.   0.  ]\n",
            "  [0.   0.   0.   0.93 0.   0.   0.   0.07]\n",
            "  [0.   0.14 0.   0.   0.82 0.   0.   0.07]\n",
            "  [0.   0.   0.   0.   0.   1.   0.   0.  ]\n",
            "  [0.   0.   0.   0.   0.   0.   1.   0.  ]\n",
            "  [0.   0.   0.   0.   0.   0.   0.   1.  ]]]\n"
          ]
        }
      ],
      "source": [
        "from sklearn.metrics import confusion_matrix\n",
        "from sklearn.model_selection import KFold\n",
        "nfold = 10\n",
        "kf = KFold(n_splits = nfold, shuffle = True)\n",
        "\n",
        "# TODO\n",
        "all_precision = []\n",
        "all_recall = []\n",
        "all_fscore = []\n",
        "all_accuracy = []\n",
        "C = []\n",
        "\n",
        "for xtr_split_indices, xts_split_indices in kf.split(X):\n",
        "  fXtr = X.iloc[xtr_split_indices, :]\n",
        "  fytr = y[xtr_split_indices]\n",
        "  fXts = X.iloc[xts_split_indices, :]\n",
        "  fyts = y[xts_split_indices]\n",
        "\n",
        "  Xtr1, Xts1 = scale(fXtr, fXts)\n",
        "  logreg = linear_model.LogisticRegression(solver = 'lbfgs', max_iter = 1000)\n",
        "  logreg.fit(fXtr, fytr)\n",
        "  yhat = logreg.predict(fXts)\n",
        "\n",
        "  precision, recall, fscore, _ = precision_recall_fscore_support(fyts, yhat, average = 'macro')\n",
        "\n",
        "  all_precision.append(precision)\n",
        "  all_recall.append(recall)\n",
        "  all_fscore.append(fscore)\n",
        "  all_accuracy.append(np.mean(fyts == yhat))\n",
        "\n",
        "  cm = confusion_matrix(fyts, yhat)\n",
        "  C.append(np.round(cm.astype('float') / cm.sum(axis = 1), 2))\n",
        "\n",
        "print(f'Average of all precision = {round(np.mean(all_precision) * 100, 2)} %')\n",
        "print(f'Average of all recall    = {round(np.mean(all_recall) * 100, 2)} %')\n",
        "print(f'Average of all f1        = {round(np.mean(all_fscore) * 100, 2)} %')\n",
        "print(f'Average of all accuracy  = {round(np.mean(all_accuracy) * 100, 2)} %')\n",
        "print(f'SE      of     accuracy  = {round(np.std(all_accuracy) / np.sqrt(len(C[0])), 2)}')\n",
        "print()\n",
        "print(np.array_str(np.array(C), precision = 4, suppress_small = True))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Lyg3uGrFEbJN"
      },
      "source": [
        "Re-run the logistic regression on the entire training data and get the weight coefficients.  This should be a 8 x 77 matrix.  Create a stem plot of the first row of this matrix to see the coefficients on each of the genes."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "RD-MjehMEbJN",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 300
        },
        "outputId": "6d896505-fd2c-4538-c745-952381171d02"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy = 97.84 %\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<StemContainer object of 3 artists>"
            ]
          },
          "metadata": {},
          "execution_count": 16
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ],
      "source": [
        "# TODO\n",
        "Xtr, Xts, ytr, yts = train_test_split(X, y, test_size = 0.3, random_state = 16)\n",
        "Xtr1, Xts1 = scale(Xtr, Xts)\n",
        "logreg.fit(Xtr1, ytr)\n",
        "\n",
        "yhat = logreg.predict(Xts1)\n",
        "print(f'Accuracy = {round(logreg.score(Xts1, yts) * 100, 2)} %')\n",
        "\n",
        "W = logreg.coef_[0]\n",
        "plt.stem(W, use_line_collection = True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "collapsed": true,
        "id": "UpvU3esREbJN"
      },
      "source": [
        "## L1-Regularization\n",
        "\n",
        "This section is bonus.\n",
        "\n",
        "In most genetic problems, only a limited number of the tested genes are likely influence any particular attribute.  Hence, we would expect that the weight coefficients in the logistic regression model should be sparse.  That is, they should be zero on any gene that plays no role in the particular attribute of interest.  Genetic analysis commonly imposes sparsity by adding an l1-penalty term.  Read the `sklearn` [documentation](http://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LogisticRegression.html) on the `LogisticRegression` class to see how to set the l1-penalty and the inverse regularization strength, `C`.\n",
        "\n",
        "Using the model selection strategies from the [housing demo](../unit05_lasso/demo2_housing.ipynb), use K-fold cross validation to select an appropriate inverse regularization strength.  \n",
        "* Use 10-fold cross validation \n",
        "* You should select around 20 values of `C`.  It is up to you find a good range.\n",
        "* Make appropriate plots and print out to display your results\n",
        "* How does the accuracy compare to the accuracy achieved without regularization."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "1_BahCXgEbJN"
      },
      "outputs": [],
      "source": [
        "# TODO\n",
        "nfold = 10\n",
        "kf = KFold(n_splits = nfold, shuffle = True)\n",
        "\n",
        "accuracy = pd.DataFrame(columns=['C IORS', 'Binary Accuracy', 'Multi Accuracy'])\n",
        "\n",
        "y_binary = np.unique(df1['Genotype'].values, return_inverse = True)[1]\n",
        "y_multi = np.unique(df1['class'], return_inverse = True)[1]\n",
        "\n",
        "all_binary_accuracy = []\n",
        "all_multi_accuracy = []\n",
        "\n",
        "C = np.logspace(-2, 2, 20)\n",
        "\n",
        "for c in C:\n",
        "  binary_accuracy = []\n",
        "  multi_accuracy = []\n",
        "  for xtr_split_indices, xts_split_indices in kf.split(X):\n",
        "    fXtr, fXts = X.iloc[xtr_split_indices, :], X.iloc[xts_split_indices, :]\n",
        "    fytr_binary, fytr_multi, fyts_binary, fyts_multi = y_binary[xtr_split_indices], y_multi[xtr_split_indices], y_binary[xts_split_indices], y_multi[xts_split_indices]\n",
        "\n",
        "    Xtr1, Xts1 = scale(fXtr, fXts)\n",
        "\n",
        "    logreg = linear_model.LogisticRegression(penalty = 'l1', C = c, solver = 'liblinear', max_iter = 1000)\n",
        "    logreg.fit(Xtr1, fytr_binary)\n",
        "    yhat_binary = logreg.predict(Xts1)\n",
        "\n",
        "    logreg.fit(Xtr1, fytr_multi)\n",
        "    yhat_multi = logreg.predict(Xts1)\n",
        "\n",
        "    binary_accuracy.append(np.mean(fyts_binary == yhat_binary))\n",
        "    multi_accuracy.append(np.mean(fyts_multi == yhat_multi))\n",
        "\n",
        "  all_binary_accuracy.append(round(np.mean(binary_accuracy) * 100, 2))\n",
        "  all_multi_accuracy.append(round(np.mean(multi_accuracy) * 100, 2))\n",
        "\n",
        "accuracy['C IORS'] = C\n",
        "accuracy['Binary Accuracy'] = all_binary_accuracy\n",
        "accuracy['Multi Accuracy'] = all_multi_accuracy"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "binary_accuracy = []\n",
        "multi_accuracy = []\n",
        "for xtr_split_indices, xts_split_indices in kf.split(X):\n",
        "  fXtr, fXts = X.iloc[xtr_split_indices, :], X.iloc[xts_split_indices, :]\n",
        "  fytr_binary, fytr_multi, fyts_binary, fyts_multi = y_binary[xtr_split_indices], y_multi[xtr_split_indices], y_binary[xts_split_indices], y_multi[xts_split_indices]\n",
        "\n",
        "  Xtr1, Xts1 = scale(fXtr, fXts)\n",
        "\n",
        "  logreg = linear_model.LogisticRegression(C = 1e5, solver = 'lbfgs', max_iter = 1000)\n",
        "  logreg.fit(Xtr1, fytr_binary)\n",
        "  yhat_binary = logreg.predict(Xts1)\n",
        "\n",
        "  logreg.fit(Xtr1, fytr_multi)\n",
        "  yhat_multi = logreg.predict(Xts1)\n",
        "\n",
        "  binary_accuracy.append(np.mean(fyts_binary == yhat_binary))\n",
        "  multi_accuracy.append(np.mean(fyts_multi == yhat_multi))"
      ],
      "metadata": {
        "id": "ihznRYxap15k"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plt.plot(accuracy['C IORS'], accuracy['Binary Accuracy'], color = 'red')\n",
        "plt.plot(accuracy['C IORS'], accuracy['Multi Accuracy'], color = 'green')\n",
        "plt.title('Comparison')\n",
        "plt.xlabel('C IORS') \n",
        "plt.ylabel('Binary Accuracy/Multi Accuracy')\n",
        "plt.legend(['Binary Accuracy', 'Multi Accuracy'])\n",
        "plt.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 295
        },
        "id": "oD0W6RaZvgta",
        "outputId": "5a7949fe-2f03-4739-eabb-ed41c84e036f"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(f\"Max     of binary accuracy with    L1 = {accuracy['Binary Accuracy'].max()} %\")\n",
        "print(f'Average of binary accuracy without L1 = {round(np.mean(binary_accuracy) * 100, 2)} %')\n",
        "print\n",
        "print(f\"Max     of multi  accuracy with    L1 = {accuracy['Multi Accuracy'].max()} %\")\n",
        "print(f'Average of multi  accuracy without L1 = {round(np.mean(multi_accuracy) * 100, 2)} %')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "D8-PL9Q8p4PY",
        "outputId": "bf4bada5-d71b-46ae-e1ed-e6932d19971b"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Max     of binary accuracy with    L1 = 97.31 %\n",
            "Average of binary accuracy without L1 = 95.09 %\n",
            "Max     of multi  accuracy with    L1 = 99.26 %\n",
            "Average of multi  accuracy without L1 = 99.44 %\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Report\n",
        "As Shown in the plot and the displayed output:\n",
        "-   Binary classifier accuracy with the L1 regularization is larger than the classifier accuracy without the L1 regularization.\n",
        "-   Multi classifier accuracy with the L1 regularization is Near to the classifier accuracy without the L1 regularization."
      ],
      "metadata": {
        "id": "XzJbcCS-ye4b"
      }
    }
  ],
  "metadata": {
    "anaconda-cloud": {},
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.4"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}